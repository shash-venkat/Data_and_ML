{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Naive Bayes classifier and LSTM for spam detection\n",
    "# Step 1 : we use the Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1ef4e64705e4489a8f32a8b1cffe714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/3.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ee76683ac6e4d75867da74c8a6bf744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/1.69k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ac0e5e8b0334cea90a05a08767a913c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/4.87k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset sms_spam (C:/Users/shash/.cache/huggingface/datasets/sms_spam/plain_text/1.0.0/53f051d3b5f62d99d61792c91acefe4f1577ad3e4c216fb0ad39e30b9f20019c)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e6964179ab04442ac2dfd73a14b9d7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#data import \n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "dataset = load_dataset(\"sms_spam\")\n",
    "df = dataset['train'].to_pandas()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean and prepare data for processing\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "a=list(df[\"sms\"])\n",
    "b=[]\n",
    "for i in a :\n",
    "    b.append(i.lower())\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def word_lemmatizer(words):\n",
    "   lemma_words = [lemmatizer.lemmatize(o) for o in words]\n",
    "   return \"\".join(lemma_words)\n",
    "\n",
    "c = []\n",
    "for i in b :\n",
    "    i = word_lemmatizer(i)\n",
    "    i = i.replace(\"1\",\"\")\n",
    "    i = i.replace(\"2\",\"\")\n",
    "    i = i.replace(\"3\",\"\")\n",
    "    i = i.replace(\"4\",\"\")\n",
    "    i = i.replace(\"5\",\"\")\n",
    "    i = i.replace(\"6\",\"\")\n",
    "    i = i.replace(\"7\",\"\")\n",
    "    i = i.replace(\"8\",\"\")\n",
    "    i = i.replace(\"9\",\"\")\n",
    "    i = i.replace(\"0\",\"\")\n",
    "    i = i.replace(\"!\",\"\")\n",
    "    i = i.replace(\"@\",\"\")\n",
    "    i = i.replace(\"#\",\"\")\n",
    "    i = i.replace(\"$\",\"\")\n",
    "    i = i.replace(\"%\",\"\")\n",
    "    i = i.replace(\"^\",\"\")\n",
    "    i = i.replace(\"&\",\"\")\n",
    "    i = i.replace(\"*\",\"\")\n",
    "    i = i.replace(\"(\",\"\")\n",
    "    i = i.replace(\")\",\"\")\n",
    "    i = i.replace(\"_\",\"\")\n",
    "    i = i.replace(\"-\",\"\")\n",
    "    i = i.replace(\"+\",\"\")\n",
    "    i = i.replace(\"=\",\"\")\n",
    "    i = i.replace(\"?\",\"\")\n",
    "    i = i.replace(\".\",\"\")\n",
    "    i = i.replace(\",\",\"\")\n",
    "    i = i.replace(\":\",\"\")\n",
    "    i = i.replace(\";\",\"\")\n",
    "    i = i.replace(\"\\'\",\"\")\n",
    "    i = i.replace(\"\\\" \",\"\")\n",
    "    i = i.replace(\"{\",\"\")\n",
    "    i = i.replace(\"}\",\"\")\n",
    "    i = i.replace(\"[\",\"\")\n",
    "    i = i.replace(\"]\",\"\")\n",
    "    i = i.replace(\"Â£\",\"\")\n",
    "    i = i.replace(\">\",\"\")\n",
    "    i = i.replace(\"\\x92s\",\"\")\n",
    "    i = i.replace(\"\\n\",\"\")\n",
    "    i = i.replace(\"/\",\"\")\n",
    "    c.append(i)\n",
    "\n",
    "clean_data = c\n",
    "data_label = list(df[\"label\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#extract features from the data\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "data_vector = vectorizer.fit_transform(clean_data)\n",
    "\n",
    "#split data into training and testing data\n",
    "x_train , x_test , y_train , y_test = tts(data_vector,data_label,test_size = 0.3, random_state = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9677226539151226\n",
      "confusion matrix: [[1440    8]\n",
      " [  46  179]]\n"
     ]
    }
   ],
   "source": [
    "#create naive bayes model to classify the data\n",
    "import sklearn.metrics\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "clf = BernoulliNB()\n",
    "clf.fit(x_train.toarray(),y_train)\n",
    "predictions = clf.predict(x_test.toarray())\n",
    "print(\"accuracy:\",clf.score(x_test.toarray(),y_test))\n",
    "print(\"confusion matrix:\",sklearn.metrics.confusion_matrix(y_test, clf.predict(x_test.toarray())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Since the Naive Bayes classifier isn't  particularly accurate\n",
    "# Especially looking at the confusion matrix\n",
    "# We now use LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing and preparing data for out model\n",
    "#tokenizer to convert the string data into numerical data\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "lb_enc = LabelEncoder()\n",
    "y = lb_enc.fit_transform(df['label'])\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(clean_data)\n",
    "text_to_sequence = tokenizer.texts_to_sequences(clean_data) \n",
    "size = len(tokenizer.word_index)+1\n",
    "#padding the data so that all vectors are of the same size\n",
    "max_length_sequence = max([len(i) for i in text_to_sequence])\n",
    "padded_sms_sequence = pad_sequences(text_to_sequence, maxlen=max_length_sequence, padding = \"pre\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "279/279 [==============================] - 49s 166ms/step - loss: 0.2038 - accuracy: 0.9374 - val_loss: 0.0689 - val_accuracy: 0.9785\n",
      "Epoch 2/5\n",
      "279/279 [==============================] - 34s 124ms/step - loss: 0.0392 - accuracy: 0.9883 - val_loss: 0.0535 - val_accuracy: 0.9857\n",
      "Epoch 3/5\n",
      "279/279 [==============================] - 35s 125ms/step - loss: 0.0182 - accuracy: 0.9962 - val_loss: 0.0644 - val_accuracy: 0.9830\n",
      "Epoch 4/5\n",
      "279/279 [==============================] - 33s 119ms/step - loss: 0.0088 - accuracy: 0.9980 - val_loss: 0.0642 - val_accuracy: 0.9865\n",
      "Epoch 5/5\n",
      "279/279 [==============================] - 32s 115ms/step - loss: 0.0182 - accuracy: 0.9946 - val_loss: 0.0768 - val_accuracy: 0.9758\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1fabc6ea910>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating the LSTM model\n",
    "import keras\n",
    "from keras.layers import Dense, Embedding, LSTM, Dropout\n",
    "from keras.models import Sequential\n",
    "\n",
    "clf = Sequential()\n",
    "clf.add(Embedding(size, 32, input_length=max_length_sequence))\n",
    "clf.add(LSTM(128))\n",
    "clf.add(Dropout(0.4))\n",
    "clf.add(Dense(units =32, activation ='relu'))\n",
    "clf.add(Dropout(0.3))\n",
    "clf.add(Dense(units =1 , activation = 'sigmoid'))\n",
    "clf.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "\n",
    "clf.fit(padded_sms_sequence, y, epochs = 5, batch_size=16 ,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 2s 37ms/step\n",
      "accuracy 0.980353661346186\n",
      "53/53 [==============================] - 3s 61ms/step\n",
      "confusion matrix: [[1445    3]\n",
      " [   6  219]]\n"
     ]
    }
   ],
   "source": [
    "x_train , x_test , y_train , y_test = tts(padded_sms_sequence,y,test_size = 0.3, random_state = 2)\n",
    "print(\"accuracy\",1-sklearn.metrics.log_loss(y_test,clf.predict(x_test)))\n",
    "\n",
    "val_arr=[]\n",
    "for i in clf.predict(x_test):\n",
    "    if i[0] > 0.5 :\n",
    "        val_arr.append(1)\n",
    "    else:\n",
    "        val_arr.append(0)\n",
    "\n",
    "print(\"confusion matrix:\",sklearn.metrics.confusion_matrix(y_test,val_arr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# As we can see the LSTM performs much better\n",
    "# This can be further improved by tweaking the model or running more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
